{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37861728",
   "metadata": {},
   "source": [
    "notes : \n",
    "- trouver comment exploiter le \"superuser\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1369ecd",
   "metadata": {},
   "source": [
    "## Traitement NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e75b0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79c8c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"data/philly_restaurant_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "textes=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "34905219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JrIxlS1TzJ-iCu79ul40cQ</td>\n",
       "      <td>eUta8W_HdHMXPzLBBZhL1A</td>\n",
       "      <td>04UD14gamNjLY0IDYVhHJg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>2015-09-23 23:10:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8JFGBuHMoiNDyfcxuWNtrA</td>\n",
       "      <td>smOvOajNG0lS4Pq7d8g4JQ</td>\n",
       "      <td>RZtGWDLCAtuipwaZ-UfjmQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good food--loved the gnocchi with marinara\\nth...</td>\n",
       "      <td>2009-10-14 19:57:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oyaMhzBSwfGgemSGuZCdwQ</td>\n",
       "      <td>Dd1jQj7S-BFGqRbApFzCFw</td>\n",
       "      <td>YtSqYv1Q_pOltsVPSx54SA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tremendous service (Big shout out to Douglas) ...</td>\n",
       "      <td>2013-06-24 11:21:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xs8Z8lmKkosqW5mw_sVAoA</td>\n",
       "      <td>IQsF3Rc6IgCzjVV9DE8KXg</td>\n",
       "      <td>eFvzHawVJofxSnD7TgbZtg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>My absolute favorite cafe in the city. Their b...</td>\n",
       "      <td>2014-11-12 15:30:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "1  JrIxlS1TzJ-iCu79ul40cQ  eUta8W_HdHMXPzLBBZhL1A  04UD14gamNjLY0IDYVhHJg   \n",
       "2  8JFGBuHMoiNDyfcxuWNtrA  smOvOajNG0lS4Pq7d8g4JQ  RZtGWDLCAtuipwaZ-UfjmQ   \n",
       "3  oyaMhzBSwfGgemSGuZCdwQ  Dd1jQj7S-BFGqRbApFzCFw  YtSqYv1Q_pOltsVPSx54SA   \n",
       "4  Xs8Z8lmKkosqW5mw_sVAoA  IQsF3Rc6IgCzjVV9DE8KXg  eFvzHawVJofxSnD7TgbZtg   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    5.0       1      0     1   \n",
       "1    1.0       1      2     1   \n",
       "2    4.0       0      0     0   \n",
       "3    5.0       0      0     0   \n",
       "4    5.0       0      0     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n",
       "1  I am a long term frequent customer of this est...  2015-09-23 23:10:31  \n",
       "2  Good food--loved the gnocchi with marinara\\nth...  2009-10-14 19:57:14  \n",
       "3  Tremendous service (Big shout out to Douglas) ...  2013-06-24 11:21:25  \n",
       "4  My absolute favorite cafe in the city. Their b...  2014-11-12 15:30:27  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce01c5",
   "metadata": {},
   "source": [
    "Nous nâ€™avons volontairement pas concatÃ©nÃ© lâ€™ensemble des avis dâ€™un restaurant afin dâ€™Ã©viter un biais de volume liÃ© au nombre dâ€™avis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f10e748c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5852.000000\n",
       "mean      117.445147\n",
       "std       247.806219\n",
       "min         5.000000\n",
       "25%        14.000000\n",
       "50%        40.000000\n",
       "75%       118.000000\n",
       "max      5778.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# nombre d'avis par business\n",
    "nb_avis = textes.groupby(\"business_id\").size()\n",
    "\n",
    "nb_avis.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d8149",
   "metadata": {},
   "source": [
    "Nous prenons 40 avis par restaurant ce qui correspond Ã  la mediane du nombre d'avis par restaurant. Nous prenons les 40 avis les plus recents pour avoir l'information la plus a jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "38e7555f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0M0b-XhtFagyLmsBtOe8w</td>\n",
       "      <td>Ok so this place is an extremely odd hole in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0PN_KFPtbnLQZEeb23XiA</td>\n",
       "      <td>Far and away the best Chinese food to order if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0TffRSXXIlBYVbb5AwfTg</td>\n",
       "      <td>I went to indieblue a few years ago for happy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0eUa8TsXFFy0FCxHYmrjg</td>\n",
       "      <td>We are sitting in an ICU waiting room, several...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1B9pP_CrRBJYPICE5WbRA</td>\n",
       "      <td>The food is absolutely amazing.    I get food ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               text\n",
       "0  -0M0b-XhtFagyLmsBtOe8w  Ok so this place is an extremely odd hole in t...\n",
       "1  -0PN_KFPtbnLQZEeb23XiA  Far and away the best Chinese food to order if...\n",
       "2  -0TffRSXXIlBYVbb5AwfTg  I went to indieblue a few years ago for happy ...\n",
       "3  -0eUa8TsXFFy0FCxHYmrjg  We are sitting in an ICU waiting room, several...\n",
       "4  -1B9pP_CrRBJYPICE5WbRA  The food is absolutely amazing.    I get food ..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s'assurer que la colonne date est bien au format datetime\n",
    "textes[\"date\"] = pd.to_datetime(textes[\"date\"])\n",
    "\n",
    "textes_restaurant = (\n",
    "    textes\n",
    "    .sort_values(\"date\", ascending=False)   # du plus rÃ©cent au plus ancien\n",
    "    .groupby(\"business_id\")\n",
    "    .head(40)                                # garder les 40 plus rÃ©cents\n",
    "    .groupby(\"business_id\", as_index=False)[\"text\"]\n",
    "    .apply(\" \".join)\n",
    ")\n",
    "\n",
    "textes_restaurant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497664e",
   "metadata": {},
   "source": [
    "ConformÃ©ment Ã  la mÃ©thodologie proposÃ©e par BERTopic, les documents sont reprÃ©sentÃ©s Ã  lâ€™aide dâ€™embeddings Sentence-BERT (SBERT), une approche spÃ©cifiquement conÃ§ue pour comparer des phrases et des documents entiers sur la base de leur similaritÃ© sÃ©mantique. d'apres les auteurs it achieves state-of-the-art performance on various sentence embedding tasks (Reimers and urevych, 2020; Thakur et al., 2020). En particulier, le modÃ¨le all-MiniLM-L6-v2 est recommandÃ© dans des contextes Ã  ressources limitÃ©es sans GPU\n",
    "\n",
    "notes : donner + d'argument prk sbert bon pour notre cas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333900d",
   "metadata": {},
   "source": [
    "Sbert doit recevoir un texte le plus priche du langage naturel donc les preprocessing lourds comme les stopword removal ou la lemmatization sont appliquÃ©s suelement dans la partie topic representations (c-TF-IDF) suivant les recos (Grootendorst (2022) and Reimers & Gurevych (2019).)\n",
    "\n",
    "par contre on sait que naturlmment nous allons avoir des clusters autour du type de nourriturz comment on l'a vu dans le papier... et on a fait le test et on avait bcp de mots liÃ©s Ã  la cuisine. donc on- va les enelever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "318b0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "FOOD_STOPWORDS = {\n",
    "    # plats / items\n",
    "    \"pizza\", \"pasta\", \"burger\", \"burrito\", \"taco\", \"tacos\",\n",
    "    \"sushi\", \"ramen\", \"noodles\", \"steak\", \"salad\", \"sandwich\", \"soup\",\n",
    "    \"dumpling\", \"dumplings\",\n",
    "    \"naan\", \"masala\", \"tikka\", \"curry\", \"pad\",\n",
    "    \"falafel\", \"hummus\", \"pita\",\n",
    "    \"cheesesteak\", \"hoagie\",\n",
    "    \"cake\", \"donut\", \"cupcake\",\n",
    "    \"coffee\", \"latte\", \"espresso\",\n",
    "    \"seafood\", \"shrimp\", \"lobster\", \"crab\",\n",
    "    \"fries\",\n",
    "\n",
    "    # ingrÃ©dients\n",
    "    \"chicken\", \"beef\", \"pork\", \"fish\", \"rice\", \"cheese\",\n",
    "\n",
    "    # cuisines / origines\n",
    "    \"italian\", \"french\", \"thai\", \"chinese\", \"japanese\",\n",
    "    \"mexican\", \"indian\", \"korean\", \"vietnamese\",\n",
    "    \"greek\", \"spanish\", \"turkish\"\n",
    "}\n",
    "\n",
    "def clean_for_embedding(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "\n",
    "    # Keep only letters and spaces\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove food-related terms (key step)\n",
    "    tokens = [\n",
    "        tok for tok in text.split()\n",
    "        if tok not in FOOD_STOPWORDS\n",
    "    ]\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "textes_restaurant[\"cleaned_text_embedding\"] = textes_restaurant[\"text\"].apply(clean_for_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ff174",
   "metadata": {},
   "source": [
    "On peut passer maintenant Ã l'embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "de2e9967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bc58c8582744598b316b91152e98d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "texts = textes_restaurant[\"cleaned_text_embedding\"].tolist()\n",
    "embeddings_txt = model.encode(\n",
    "    texts,\n",
    "    batch_size=32,          \n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True  #ce sera pratique pour le clustering plus tard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c3c281fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07339897, -0.02592888,  0.04090234, ..., -0.01813973,\n",
       "        -0.08050999, -0.04733944],\n",
       "       [ 0.02709788, -0.00823581,  0.05498442, ..., -0.00430089,\n",
       "        -0.08237864,  0.02010359],\n",
       "       [ 0.04485257, -0.04540356,  0.06240261, ..., -0.01022023,\n",
       "        -0.07408096, -0.02439085],\n",
       "       ...,\n",
       "       [-0.03398207, -0.0067131 ,  0.07501908, ..., -0.00645372,\n",
       "        -0.07401868, -0.02476408],\n",
       "       [-0.01100712, -0.03541555,  0.05553192, ...,  0.01721645,\n",
       "         0.01522463, -0.0668183 ],\n",
       "       [ 0.04314486,  0.022509  ,  0.12038537, ..., -0.02080076,\n",
       "        -0.06830619,  0.01881207]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938a4f3",
   "metadata": {},
   "source": [
    "1) ajouter autres features (5/6 max)\n",
    "2)RÃ©duction de dimension (UMAP)\n",
    "3)Clustering (HDBSCAN)\n",
    "\n",
    "4)Extraction des mots-clÃ©s (c-TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59ac7b",
   "metadata": {},
   "source": [
    "## Reduction de dimensionalitÃ©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea53540c",
   "metadata": {},
   "source": [
    "On utilise la methode UMAP pour la reduction de dimensionnalitÃ© â€œAlthough PCA and t-SNE are well-known methods for reducing dimensionality, UMAP has shown to preserve more of the local and global features of high-dimensional data in lower projected dimensions.â€ BERTopic\n",
    "\n",
    "on ne peut pas optimiser les hyperparametres dans du non supervisÃ© donc pour choisir le hyperparametres on se base sur bertopic, ils ont testÃ© sur plusieurs textes et ont choisir ces params. on verifiera plus tard la robustesse.\n",
    "\n",
    "notes : chercher ce que signifie chaque param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d8e4844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.00,\n",
    "    n_components=5,\n",
    "    metric=\"cosine\", # a changer apres quand on ajoutes les features numeriques par euclidean\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_umap = umap_model.fit_transform(embeddings_txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc5142",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0ee67",
   "metadata": {},
   "source": [
    "On utilise HDBscan pour effectuer la clusterisation, car deja ca focntionne bien apres umap et c'est plsu ronuste. aussi bertopic l'ont choisit lol (Ã  justifier un peu plus)\n",
    "justifier le choix d'hyperparam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07f86e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=50, #taille minimale d'un cluster, on met 50 puis on verra \n",
    "    min_samples=50, #en pratique on met min_samples = min_cluster_size pour des resultats plus stables\n",
    "    metric=\"euclidean\", # parce que UMAP nous a donnÃ© des vecteurs en euclidean\n",
    "    cluster_selection_method=\"eom\", #on garde les clusters qui ont le plus de â€œmasseâ€ stable dans lâ€™arbre, leaf npous donnerait des clusters trop petit\n",
    "    prediction_data=True #pour pouvoir assigner de nouveaux points plus tard\n",
    ")\n",
    "\n",
    "cluster_labels = hdbscan_model.fit_predict(X_umap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b6f04e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  1 10 ... -1 -1  6]\n"
     ]
    }
   ],
   "source": [
    "print(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780ff11",
   "metadata": {},
   "source": [
    "## interpretation des clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8d782",
   "metadata": {},
   "source": [
    "on commence par attacher les clusters aux textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "92437792",
   "metadata": {},
   "outputs": [],
   "source": [
    "textes_restaurant['cluster'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6f99aef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text_embedding</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0M0b-XhtFagyLmsBtOe8w</td>\n",
       "      <td>Ok so this place is an extremely odd hole in t...</td>\n",
       "      <td>ok so this place is an extremely odd hole in t...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0PN_KFPtbnLQZEeb23XiA</td>\n",
       "      <td>Far and away the best Chinese food to order if...</td>\n",
       "      <td>far and away the best food to order if you liv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0TffRSXXIlBYVbb5AwfTg</td>\n",
       "      <td>I went to indieblue a few years ago for happy ...</td>\n",
       "      <td>i went to indieblue a few years ago for happy ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0eUa8TsXFFy0FCxHYmrjg</td>\n",
       "      <td>We are sitting in an ICU waiting room, several...</td>\n",
       "      <td>we are sitting in an icu waiting room several ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1B9pP_CrRBJYPICE5WbRA</td>\n",
       "      <td>The food is absolutely amazing.    I get food ...</td>\n",
       "      <td>the food is absolutely amazing i get food deli...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5847</th>\n",
       "      <td>zxY4DgtXsVHihSUpsmwamg</td>\n",
       "      <td>This Place has Good Hoagies and the Staff is s...</td>\n",
       "      <td>this place has good hoagies and the staff is s...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>zy7uNOvpykrq-XlmDY_wHA</td>\n",
       "      <td>Sadiki's is now Relish, nothing last's over th...</td>\n",
       "      <td>sadiki s is now relish nothing last s over the...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5849</th>\n",
       "      <td>zyMkbavgHASQtqVwaock9A</td>\n",
       "      <td>I've had pretty much everything on the menu an...</td>\n",
       "      <td>i ve had pretty much everything on the menu an...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5850</th>\n",
       "      <td>zz-fcqurtm77bZ_rVvo2Lw</td>\n",
       "      <td>My love affair with Yumtown began with \"The Jo...</td>\n",
       "      <td>my love affair with yumtown began with the joy...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>zzyx5x0Z7xXWWvWnZFuxlQ</td>\n",
       "      <td>Maybe the pizza is good here... but I can real...</td>\n",
       "      <td>maybe the is good here but i can really only s...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5852 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id  \\\n",
       "0     -0M0b-XhtFagyLmsBtOe8w   \n",
       "1     -0PN_KFPtbnLQZEeb23XiA   \n",
       "2     -0TffRSXXIlBYVbb5AwfTg   \n",
       "3     -0eUa8TsXFFy0FCxHYmrjg   \n",
       "4     -1B9pP_CrRBJYPICE5WbRA   \n",
       "...                      ...   \n",
       "5847  zxY4DgtXsVHihSUpsmwamg   \n",
       "5848  zy7uNOvpykrq-XlmDY_wHA   \n",
       "5849  zyMkbavgHASQtqVwaock9A   \n",
       "5850  zz-fcqurtm77bZ_rVvo2Lw   \n",
       "5851  zzyx5x0Z7xXWWvWnZFuxlQ   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Ok so this place is an extremely odd hole in t...   \n",
       "1     Far and away the best Chinese food to order if...   \n",
       "2     I went to indieblue a few years ago for happy ...   \n",
       "3     We are sitting in an ICU waiting room, several...   \n",
       "4     The food is absolutely amazing.    I get food ...   \n",
       "...                                                 ...   \n",
       "5847  This Place has Good Hoagies and the Staff is s...   \n",
       "5848  Sadiki's is now Relish, nothing last's over th...   \n",
       "5849  I've had pretty much everything on the menu an...   \n",
       "5850  My love affair with Yumtown began with \"The Jo...   \n",
       "5851  Maybe the pizza is good here... but I can real...   \n",
       "\n",
       "                                 cleaned_text_embedding  cluster  \n",
       "0     ok so this place is an extremely odd hole in t...       10  \n",
       "1     far and away the best food to order if you liv...        1  \n",
       "2     i went to indieblue a few years ago for happy ...       10  \n",
       "3     we are sitting in an icu waiting room several ...       -1  \n",
       "4     the food is absolutely amazing i get food deli...        4  \n",
       "...                                                 ...      ...  \n",
       "5847  this place has good hoagies and the staff is s...        6  \n",
       "5848  sadiki s is now relish nothing last s over the...       -1  \n",
       "5849  i ve had pretty much everything on the menu an...       -1  \n",
       "5850  my love affair with yumtown began with the joy...       -1  \n",
       "5851  maybe the is good here but i can really only s...        6  \n",
       "\n",
       "[5852 rows x 4 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textes_restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "101f41e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3154"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(textes_restaurant[\"cluster\"] == -1).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bdc6e",
   "metadata": {},
   "source": [
    "ca fait 30% d'outliers, c'est beaucoup j'ai l'impression ? peut etre reduire le nbre de documents minimum par cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4ede4",
   "metadata": {},
   "source": [
    "Le but de cette prochaine etape est de labeliser les clusters. pour cela on va dabord faire un nettoyage des textes plus important que pour le Sbert. En effet sbert comprends le sens total de la phrase, la pou c-TFIDF travailles avec des mots discrimiannt.\n",
    "Etant donnÃ© que nous on ne veut pas avoir des clusters qui se concentrent sur le type de nourriture mais sur l'experience on va enlever les mots relatifs aux plats, pour eviter d'avoir des cluysters de type \"chinease food\" \"indian food\" ect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ff5cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stoplist = set(stopwords.words(\"english\"))\n",
    "porter = PorterStemmer()\n",
    "FOOD_STOPWORDS = {\n",
    "    # plats / items\n",
    "    \"pizza\", \"pasta\", \"burger\", \"burrito\", \"taco\", \"tacos\",\n",
    "    \"sushi\", \"ramen\", \"noodles\", \"steak\", \"salad\", \"sandwich\", \"soup\",\n",
    "    \"dumpling\", \"dumplings\",\n",
    "    \"naan\", \"masala\", \"tikka\", \"curry\", \"pad\",\n",
    "    \"falafel\", \"hummus\", \"pita\",\n",
    "    \"cheesesteak\", \"hoagie\",\n",
    "    \"cake\", \"donut\", \"cupcake\",\n",
    "    \"coffee\", \"latte\", \"espresso\",\n",
    "    \"seafood\", \"shrimp\", \"lobster\", \"crab\",\n",
    "    \"fries\",\n",
    "\n",
    "    # ingrÃ©dients\n",
    "    \"chicken\", \"beef\", \"pork\", \"fish\", \"rice\", \"cheese\",\n",
    "\n",
    "    # cuisines / origines\n",
    "    \"italian\", \"french\", \"thai\", \"chinese\", \"japanese\",\n",
    "    \"mexican\", \"indian\", \"korean\", \"vietnamese\",\n",
    "    \"greek\", \"spanish\", \"turkish\"\n",
    "}\n",
    "stoplist = stoplist.union(FOOD_STOPWORDS)\n",
    "\n",
    "\n",
    "def clean_for_ctfidf(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    tokens = [\n",
    "        porter.stem(tok)\n",
    "        for tok in text.split()\n",
    "        if tok not in stoplist and len(tok) > 2\n",
    "    ]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "textes_restaurant[\"cleaned_text_ctfidf\"] = textes_restaurant[\"text\"].apply(clean_for_ctfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7b9ce",
   "metadata": {},
   "source": [
    "on fait un document avec tous les textes par cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d39c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = textes_restaurant.copy()\n",
    "\n",
    "docs_per_cluster = (\n",
    "    df[df[\"cluster\"] != -1]\n",
    "    .groupby(\"cluster\")[\"cleaned_text_ctfidf\"]\n",
    "    .apply(\" \".join)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a343ab",
   "metadata": {},
   "source": [
    "on calcul le c-tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1aff9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# vectorisation\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(docs_per_cluster.values)\n",
    "\n",
    "# c-TF-IDF\n",
    "tf = X.toarray()\n",
    "tf_sum = tf.sum(axis=0)\n",
    "\n",
    "A = tf.sum(axis=1).mean()\n",
    "\n",
    "ctfidf = tf * np.log(1 + A / tf_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b56cc7",
   "metadata": {},
   "source": [
    "on extrait les tops mots pas cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ad888faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "top_words = {}\n",
    "\n",
    "for i, cluster_id in enumerate(docs_per_cluster.index):\n",
    "    scores = ctfidf[i]\n",
    "    top_idx = scores.argsort()[::-1][:10]\n",
    "    top_words[cluster_id] = [(terms[j], scores[j]) for j in top_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "02d93e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¦ Cluster 0\n",
      "  vegan                     9121.134\n",
      "  food                      4939.191\n",
      "  good                      3854.357\n",
      "  place                     3818.523\n",
      "  order                     3292.073\n",
      "  tri                       3282.370\n",
      "  great                     3262.118\n",
      "  love                      3089.146\n",
      "  delici                    2997.990\n",
      "  like                      2837.979\n",
      "\n",
      "ðŸŸ¦ Cluster 1\n",
      "  food                      28754.853\n",
      "  order                     23990.038\n",
      "  place                     21493.280\n",
      "  good                      20749.405\n",
      "  roll                      16997.314\n",
      "  like                      15843.372\n",
      "  time                      14950.833\n",
      "  restaur                   14642.778\n",
      "  tri                       14642.122\n",
      "  great                     13963.361\n",
      "\n",
      "ðŸŸ¦ Cluster 2\n",
      "  food                      7536.471\n",
      "  good                      5541.074\n",
      "  place                     5342.325\n",
      "  order                     5198.237\n",
      "  salsa                     4370.295\n",
      "  great                     4177.005\n",
      "  tri                       3940.040\n",
      "  delici                    3874.326\n",
      "  like                      3647.299\n",
      "  flavor                    3459.749\n",
      "\n",
      "ðŸŸ¦ Cluster 3\n",
      "  food                      5912.911\n",
      "  order                     4205.219\n",
      "  servic                    4166.136\n",
      "  place                     4156.645\n",
      "  good                      3986.785\n",
      "  restaur                   3790.204\n",
      "  time                      3520.638\n",
      "  like                      3280.157\n",
      "  tabl                      3267.967\n",
      "  came                      3006.865\n",
      "\n",
      "ðŸŸ¦ Cluster 4\n",
      "  order                     32111.204\n",
      "  food                      22435.014\n",
      "  time                      16927.563\n",
      "  place                     16651.375\n",
      "  good                      14009.507\n",
      "  like                      13549.406\n",
      "  servic                    12819.106\n",
      "  custom                    11729.060\n",
      "  wait                      10858.545\n",
      "  ask                       10182.105\n",
      "\n",
      "ðŸŸ¦ Cluster 5\n",
      "  wing                      6976.135\n",
      "  order                     3758.360\n",
      "  place                     2441.240\n",
      "  good                      2397.647\n",
      "  food                      2360.597\n",
      "  time                      1886.998\n",
      "  sauc                      1761.567\n",
      "  like                      1690.996\n",
      "  great                     1627.058\n",
      "  tri                       1375.128\n",
      "\n",
      "ðŸŸ¦ Cluster 6\n",
      "  order                     9824.409\n",
      "  good                      8726.320\n",
      "  place                     8536.450\n",
      "  pie                       8258.927\n",
      "  slice                     7250.000\n",
      "  crust                     6312.742\n",
      "  like                      6276.248\n",
      "  time                      6059.508\n",
      "  great                     6014.781\n",
      "  tri                       5772.547\n",
      "\n",
      "ðŸŸ¦ Cluster 7\n",
      "  donut                     5876.075\n",
      "  bakeri                    4613.986\n",
      "  chocol                    3703.256\n",
      "  cooki                     3678.376\n",
      "  good                      2681.090\n",
      "  cupcak                    2629.067\n",
      "  tri                       2543.986\n",
      "  bun                       2540.891\n",
      "  sweet                     2520.544\n",
      "  like                      2430.639\n",
      "\n",
      "ðŸŸ¦ Cluster 8\n",
      "  breakfast                 7204.503\n",
      "  food                      4478.283\n",
      "  egg                       4377.624\n",
      "  place                     3811.761\n",
      "  good                      3622.027\n",
      "  great                     3267.453\n",
      "  order                     3037.342\n",
      "  toast                     2795.023\n",
      "  servic                    2700.927\n",
      "  time                      2594.293\n",
      "\n",
      "ðŸŸ¦ Cluster 9\n",
      "  place                     4388.822\n",
      "  cafe                      4103.600\n",
      "  shop                      3732.820\n",
      "  great                     3587.530\n",
      "  good                      3519.802\n",
      "  work                      3191.524\n",
      "  like                      2837.979\n",
      "  friendli                  2620.288\n",
      "  nice                      2553.854\n",
      "  realli                    2487.122\n",
      "\n",
      "ðŸŸ¦ Cluster 10\n",
      "  bar                       27544.135\n",
      "  drink                     22803.931\n",
      "  beer                      22236.048\n",
      "  place                     19171.510\n",
      "  great                     18087.020\n",
      "  food                      17614.164\n",
      "  good                      17475.873\n",
      "  time                      13815.995\n",
      "  bartend                   13143.763\n",
      "  like                      12831.202\n",
      "\n",
      "ðŸŸ¦ Cluster 11\n",
      "  restaur                   6324.657\n",
      "  food                      6186.965\n",
      "  great                     5299.942\n",
      "  good                      5032.271\n",
      "  wine                      4874.812\n",
      "  menu                      4739.215\n",
      "  servic                    4698.672\n",
      "  dinner                    4381.217\n",
      "  dish                      4303.174\n",
      "  place                     4098.037\n"
     ]
    }
   ],
   "source": [
    "for cluster_id, words in top_words.items():\n",
    "    print(f\"\\nðŸŸ¦ Cluster {cluster_id}\")\n",
    "    for word, score in words:\n",
    "        print(f\"  {word:<25} {score:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
